---
title: "Final Prediction Postmortem"
author: "Sammy Duggasani"
date: "2024-11-13"
slug: []
categories: []
tags: []
---

# **Final Prediction Postmortem**

**Monday, November 17, 2024**\
**2 Weeks since Presidential Election**

*It's been two weeks since Donald Trump won the 2024 Presidential Election. If you remember my prediction, you'll notice how this is not what my model ended up forecasting. The past few days have seen Democratic strategists scrambling to figure out where they fell short. This week, I'll be doing much the same with regards to my model. We'll focus on what my model got wrong and attempt to theorize what it actually did well â€” despite an incorrect prediction.*

## Model Recap

As a reminder of my final model, I ultimately decided to go with a LASSO regression. I thought it was best to go with a conservative approach to building my model because so many election forecasts run into the issue of overfitting: they take in too much information that might not necessarily be significant on vote behavior and spin patterns out of the data that might not exist. LASSO was attractive to me because it wholly nullifies those predictions that are not as influential on the response variable and selects only those *most* relevant.

```{r warning=FALSE, include=FALSE}
####----------------------------------------------------------#
#### Preamble
####----------------------------------------------------------#

# Load libraries.
## install via `install.packages("name")`
library(geofacet)
library(ggpubr)
library(ggthemes)
library(haven)
library(kableExtra)
library(maps)
library(mgcv)
library(mgcViz)
library(RColorBrewer)
library(scales)
library(sf)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)
library(modelsummary)

## Import custom themes
map_theme <- function() {
  theme(
    # Clean up border and background
    panel.border = element_blank(),
    panel.background = element_rect(fill = "snow2", color = NA),
    # Center and bold title, include subtitle options
    plot.title = element_text(size = 15, hjust = .5, face = "bold", family = "sans"),
    plot.subtitle = element_text(size = 13, hjust = .5, family = "sans"),
    plot.title.position = "panel",
    # Remove extraneous information and clean up
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.line = element_blank(),
    # Legend 
    legend.position = "right",
    legend.title = element_text(size = 12, family = "sans"),
    legend.text = element_text(size = 10, family = "sans"),
    # Aspect ratio
    aspect.ratio = .8
  )
}
plot_date_theme <- function() {
  theme(
    # no border
    panel.border = element_blank(),
    # background
    panel.background = element_rect(fill = "snow2"),
    # text
    plot.title = element_text(size = 15, hjust = .5, face = "bold", family = "sans"),
    plot.subtitle = element_text(size = 13, hjust = .5, family = "sans"),
    plot.title.position = "panel",
    axis.text.x = element_text(size = 8, family = "sans"),
    axis.text.y = element_text(size = 8, family = "sans"),
    axis.title.x = element_text(family = "sans"),
    axis.title.y = element_text(angle = 90, family = "sans"),
    axis.ticks = element_line(colour = "black"),
    axis.line = element_line(colour = "grey"),
    # legend 
    legend.position = "right",
    legend.title = element_text(size = 12, family = "sans"),
    legend.text = element_text(size = 10, family = "sans"),
  )
}
plot_theme <- function() {
  theme(
    # no border
    panel.border = element_blank(),
    # background
    panel.background = element_rect(fill = "snow2"),
    # text
    plot.title = element_text(size = 15, hjust = .5, face = "bold", family = "sans"),
    plot.subtitle = element_text(size = 13, hjust = .5, family = "sans"),
    plot.title.position = "panel",
    axis.text.x = element_text(size = 8, family = "sans"),
    axis.text.y = element_text(size = 8, family = "sans"),
    axis.title.x = element_text(family = "sans"),
    axis.title.y = element_text(angle = 90, family = "sans"),
    axis.ticks = element_line(colour = "black"),
    axis.line = element_line(colour = "grey"),
    # legend 
    legend.position = "right",
    legend.title = element_text(size = 12, family = "sans"),
    legend.text = element_text(size = 10, family = "sans"),
  )
}
my_prettier_theme <- function() {
  theme(
    # no border
    panel.border = element_blank(),
    # background
    panel.background = element_rect(fill = "snow2"),
    # text
    plot.title = element_text(size = 15, hjust = .5, face = "bold", family = "sans"),
    plot.subtitle = element_text(size = 13, hjust = .5, family = "sans"),
    plot.title.position = "panel",
    axis.text.x = element_text(size = 8, angle = 90, hjust = .5, family = "sans"),
    axis.text.y = element_text(size = 8, family = "sans"),
    axis.title.x = element_text(family = "sans"),
    axis.title.y = element_text(angle = 90, family = "sans"),
    axis.ticks = element_line(colour = "black"),
    axis.line = element_line(colour = "grey"),
    # legend 
    legend.position = "right",
    legend.title = element_text(size = 12, family = "sans"),
    legend.text = element_text(size = 10, family = "sans"),
    # aspect ratio
    # aspect.ratio = .8
  )
}

# Code to color cells based on winner
color_state <- function(winner, state_name) {
  if (winner == "Democrat") {
    return(cell_spec(state_name, "html", color = "white", background = "steelblue3"))
  } else if (winner == "Republican") {
    return(cell_spec(state_name, "html", color = "white", background = "tomato3"))
  } else {
    return(state_name)
  }
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
####----------------------------------------------------------#
#### Read, merge, and process data. -- Code by Matthew Dardet
####----------------------------------------------------------#

# Read popular vote datasets. 
d_popvote <- read_csv("popvote_1948_2020.csv", show_col_types = FALSE)
d_state_popvote <- read_csv("state_popvote_1948_2020.csv")
d_state_popvote[d_state_popvote$state == "District of Columbia",]$state <- "District Of Columbia"

# Read elector distribution dataset. 
d_ec <- read_csv("corrected_ec_1948_2024.csv", show_col_types = FALSE)

# Read polling data. 
d_polls <- read_csv("national_polls_1968-2024.csv", show_col_types = FALSE)
d_state_polls <- read_csv("state_polls_1968-2024.csv", show_col_types = FALSE)

# Process state-level polling data. 
d_pollav_state <- d_state_polls |> 
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support, na.rm = TRUE)) |>
  top_n(1, poll_date) |> 
  rename(latest_pollav = poll_support) |>
  select(-c(weeks_left, days_left, poll_date, candidate, before_convention)) |>
  pivot_wider(names_from = party, values_from = c(latest_pollav, mean_pollav))

# Read turnout data. 
d_turnout <- read_csv("state_turnout_1980_2022.csv", show_col_types = FALSE)

# Read county turnout. 
d_county_turnout <- read_csv("county_turnout.csv", show_col_types = FALSE)

# Read state-level demographics.
d_state_demog <- read_csv("demographics.csv", show_col_types = FALSE)

# Read county demographics. 
d_county_demog <- read_csv("county_demographics.csv", show_col_types = FALSE)

# Read campaign events datasets. 
d_campaign_events <- read_csv("campaigns_2016_2024.csv", show_col_types = FALSE)[,-1]

# Read economic data.
d_econ <- read_csv("fred_econ.csv", show_col_types = FALSE) |> 
  filter(quarter == 2)

# Read campaign spending
campaign_spending <- read_csv("FEC_contributions_by_state_2008_2024.csv", show_col_types = FALSE)

# Read turnout data
d_state_turnout <- read_csv("state_turnout_1980_2022.csv", show_col_types = FALSE)
d_state_turnout <- d_state_turnout |> 
  mutate(vep_turnout = as.numeric(str_remove(vep_turnout, "%"))/100) |> 
  select(year, state, vep_turnout)
```

```{r message=FALSE, cache=TRUE, include=FALSE}
####--------------------------------------------------------------#
#### Update model
####--------------------------------------------------------------#
set.seed(02138)

d_campaign_spending <- d_state_popvote |> 
  mutate(state_abb = state.abb[match(d_state_popvote$state, state.name)]) |> 
  left_join(campaign_spending |> filter(party == "Democrat"), by = c("year" = "election_year", "state_abb" = "contribution_state")) |> 
  filter(year >= 2008)

composite_dataset <- d_pollav_state |>
  left_join(d_econ, by = "year") |>
  left_join(d_popvote |>
              filter(party == "democrat"), 
            by = "year") |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  left_join(d_state_turnout, by = c("year", "state")) |>
  left_join(d_campaign_spending |>
              select(year, state, contribution_receipt_amount), by = c("year", "state")) |>
  filter(year >= 2008) |>
  ungroup()

# Only select and train on battleground states
battleground_states = list("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")

composite_dataset <- composite_dataset |>
  filter(state %in% battleground_states)

# Split into train and test data
d_train <- composite_dataset |>
  filter(year < 2024)
d_test <- composite_dataset |>
  filter(year == 2024)

# Create a model that involves turnout and economic indicators from previous models
simp.vars <- c("D_pv2p_lag1", "D_pv2p_lag2", "latest_pollav_DEM", "mean_pollav_DEM",
               "CPI", "GDP_growth_quarterly", "contribution_receipt_amount")

# mod_lm_dem_simp <- lm(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + contribution_receipt_amount,
#                       data = d_train)

# Add back in lagged vote share for 2024. 
t <- composite_dataset |> 
  filter(year >= 2016) |> 
  arrange(year) |> 
  group_by(state) |> 
  mutate(
    D_pv2p_lag1 = lag(D_pv2p, 1),
    D_pv2p_lag2 = lag(D_pv2p, 2)) |> 
  filter(year == 2024) |> 
  select(state, year, D_pv2p, D_pv2p_lag1, D_pv2p_lag2)

# Subset testing data to only relevant variables for our simple model. 
d_test_simp <- d_test |> 
  select(-c(D_pv2p, D_pv2p_lag1, D_pv2p_lag2)) |> 
  left_join(t, by = c("state", "year")) |> 
  select(state, year, all_of(simp.vars))

# Get average state-level turnout across 2020, 2016, 2012.  
d_donation_avg <- d_train |> 
  filter(year %in% c(2020, 2016, 2012)) |> 
  filter(state %in% unique(d_test_simp$state)) |> 
  group_by(state) |> 
  summarize(contribution_receipt_amount = mean(contribution_receipt_amount, na.rm = TRUE))

# Make predictions with simple average turnout. 
d_test_simp <- d_test_simp |> 
  left_join(d_donation_avg, by = "state") |> 
  select(-contribution_receipt_amount.x) |> 
  rename(contribution_receipt_amount = contribution_receipt_amount.y)
```

```{r echo=FALSE, cache=TRUE}
library(glmnet)
set.seed(02138)
# LASSO regression
x_train <- model.matrix(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + log(contribution_receipt_amount), data = d_train)[,-1]
y_train <- d_train$D_pv2p
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)

# Make predictions
x_test <- model.matrix(~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + log(contribution_receipt_amount), data = d_test_simp)[,-1]
simp_pred_dem <- predict(lasso_model, newx = x_test, s = "lambda.min")
simp_pred_rep <- 100 - simp_pred_dem

# Make a battleground dataset
pred_dem <- data.frame(
  State = c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"),
  Democrat = simp_pred_dem
) |>
  rename(Democrat = lambda.min)

pred_rep <- data.frame(
  State = c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"),
  Republican = simp_pred_rep
) |>
  rename(Republican = lambda.min)

predictions <- pred_dem |>
  left_join(pred_rep, by = "State") |>
  mutate(Winner = if_else(Democrat>Republican, "Democrat", "Republican"))

battleground_win_pred <- data.frame(state = predictions$State,
                       year = rep(2024, length(predictions$State)),
                       Democrat = predictions$Democrat,
                       Republican = predictions$Republican,
                       winner = ifelse(predictions$Winner=="Democrat", "Democrat", "Republican")) |>
  left_join(d_ec, by = c("state", "year")) |>
  select(-year, -stateab) |>
  select(state, electors, Democrat, Republican, winner)
```

![](lasso_coef.png)

This is just a reminder of the coefficients of each variable involved in the LASSO regression. As you can see, the LASSO model nullifies the mean Democratic poll average, and does not consider it to be a relevant predictor for the response variable of Democratic 2-Party vote share. LASSO also notably diminishes the significance of the Consumer Price Score and GDP Growth variables as compared to previous models.

```{r echo=FALSE}
options(warn = -1)
library(glmnet)
library(boot)

# Bootstrapping function 
bootstrap_fn <- function(data, indices) {
  d <- data[indices, ]
  x_boot <- model.matrix(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + log(contribution_receipt_amount), data = d)[, -1]
  y_boot <- d$D_pv2p
  fit <- cv.glmnet(x_boot, y_boot, alpha = 1)
  as.vector(coef(fit, s = "lambda.min"))
}

data_for_bootstrap <- d_train

# Bootstrapping the coefficients
set.seed(02138)
boot_results <- boot(data_for_bootstrap, bootstrap_fn, R = 1000)

# Calculate confidence intervals for all coefficients
coef_names <- rownames(coef(lasso_model, s = "lambda.min"))
coef_cis <- lapply(1:length(coef_names), function(i) {
  boot.ci(boot_results, type = "perc", index = i)
})
names(coef_cis) <- coef_names

# coef_cis

coef_summary <- do.call(rbind, lapply(1:length(coef_names), function(i) {
  ci <- boot.ci(boot_results, type = "perc", index = i)
  c(
    Predictor = coef_names[i],
    Lower = ci$percent[4],  # Lower bound of CI
    Upper = ci$percent[5]   # Upper bound of CI
  )
}))

coef_summary <- as.data.frame(coef_summary)
coef_summary$Lower <- as.numeric(coef_summary$Lower)
coef_summary$Upper <- as.numeric(coef_summary$Upper)
coef_summary |>
  kable(caption = "Coefficients about a 95% Confidence Interval") |>
  kable_styling("striped")
```

With bootstrapping, I got 95% confidence intervals for each variable and found most of them to include 0, which brought into question their relevance to the response variable. Nevertheless, I trusted LASSO to select those features which were most relevant to the model.

```{r echo=FALSE}
set.seed(02138)

# Predict on the test set
simp_pred_dem <- predict(lasso_model, newx = x_test, s = "lambda.min")
simp_pred_rep <- 100 - simp_pred_dem

# Residuals for the predictions
residuals <- y_train - predict(lasso_model, newx = x_train, s = "lambda.min")
residual_sd <- sd(residuals)

# Simulate predictions to generate confidence intervals
m <- 1000  
pred_mat <- replicate(m, {
  noise <- rnorm(length(simp_pred_dem), mean = 0, sd = residual_sd)
  predicted_dem <- simp_pred_dem + noise
  predicted_rep <- 100 - predicted_dem
  data.frame(Democrat = predicted_dem, Republican = predicted_rep)
}, simplify = FALSE)

# Combine all simulations
pred_sim <- do.call(rbind, pred_mat)

# Group by state and compute confidence intervals
pred_sim_summary <- pred_sim |>
  mutate(state = rep(d_test_simp$state, m)) |>
  group_by(state) |>
  summarize(
    mean_dem = mean(lambda.min),
    sd_dem = sd(lambda.min),
    lower_dem = mean_dem - 1.96 * sd_dem,
    upper_dem = mean_dem + 1.96 * sd_dem,
    mean_rep = mean(lambda.min.1),
    sd_rep = sd(lambda.min.1),
    lower_rep = mean_rep - 1.96 * sd_rep,
    upper_rep = mean_rep + 1.96 * sd_rep
  )
pred_sim_summary <- pred_sim_summary |>
  mutate(
    winner = ifelse(mean_dem > mean_rep, "Democrat", "Republican"),
    colored_state = mapply(color_state, winner, state)
  )

# Colored prediction table
pred_sim_summary |>
  select(colored_state, mean_dem, sd_dem, lower_dem, upper_dem, mean_rep, sd_rep, lower_rep, upper_rep) |>
  rename(State = colored_state) |>
  kable("html", escape = FALSE) |>
  kable_styling("striped")
```

Here, I have a chart of my vote share predictions for each battleground state and expected Michigan, Nevada, Pennsylvania, and Wisconsin to go to Harris while the rest (Arizona, Georgia, and North Carolina) would go to Trump. It is important to note, however, that for each state the win margin for each party was well within margin of error for a 95% confidence interval. This means that both Harris and Trump had a chance of winning each of the battleground states, according to my model.

## Election Recap

```{r include=FALSE}
options(warn = -1)
options(verbose = FALSE)
# Following chunk, code provided by Matthew Dardet

####----------------------------------------------------------#
#### Preamble
####----------------------------------------------------------#

# Load libraries.
library(censable)
library(geofacet)
library(ggpubr)
library(ggthemes)
library(haven)
library(kableExtra)
library(maps)
library(mgcv)
library(mgcViz)
library(RColorBrewer)
library(readstata13)
library(scales)
library(sf)
library(spData)
library(stargazer)
library(tidygeocoder)
library(tidyverse)
library(tigris)
library(tmap)
library(tmaptools)
library(viridis)

####-------------------------------------------------------------------------#
#### Read, merge, and process data.
####-------------------------------------------------------------------------#

# Read 2024 results datasets. 
setwd("/Users/sammy/Documents/Harvard/Senior_Year/GOV1347/Week 10")
d_state_2024 <- read_csv("state_votes_pres_2024.csv", show_col_types = FALSE)[-1, 1:6]
d_county_2024 <- read_csv("county_votes_pres_2024.csv", show_col_types = FALSE)[-1, 1:6]
d_county_2020 <- read_csv("county_votes_pres_2020.csv", show_col_types = FALSE)[-1, 1:6]

# Process 2024 state and county-level data. 
d_state_2024 <- d_state_2024 |> 
  mutate(FIPS = as.numeric(FIPS), 
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2024 <- d_county_2024 |>
  mutate(FIPS = as.numeric(FIPS),
         votes_trump = as.numeric(`Donald J. Trump`), 
         votes_harris = as.numeric(`Kamala D. Harris`), 
         votes = as.numeric(`Total Vote`), 
         trump_pv = votes_trump/votes, 
         harris_pv = votes_harris/votes, 
         trump_2pv = votes_trump/(votes_trump + votes_harris), 
         harris_2pv = votes_harris/(votes_trump + votes_harris)) |> 
  mutate(winner = case_when(votes_trump > votes_harris ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump, votes_harris, votes, 
         winner, trump_pv, harris_pv, trump_2pv, harris_2pv)

d_county_2020 <- d_county_2020 |> 
  mutate(FIPS = as.numeric(FIPS),
         votes_trump_2020 = as.numeric(`Donald J. Trump`), 
         votes_biden_2020 = as.numeric(`Joseph R. Biden Jr.`), 
         votes_2020 = as.numeric(`Total Vote`), 
         trump_pv_2020 = votes_trump_2020/votes_2020, 
         biden_pv_2020 = votes_biden_2020/votes_2020, 
         trump_2pv_2020 = votes_trump_2020/(votes_trump_2020 + votes_biden_2020), 
         biden_2pv_2020 = votes_biden_2020/(votes_trump_2020 + votes_biden_2020)) |> 
  mutate(winner_2020 = case_when(votes_trump_2020 > votes_biden_2020 ~ "REP", 
                            .default = "DEM")) |> 
  select(FIPS, `Geographic Name`, `Geographic Subtype`, votes_trump_2020, votes_biden_2020, votes_2020, 
         winner_2020, trump_pv_2020, biden_pv_2020, trump_2pv_2020, biden_2pv_2020)

####-------------------------------------------------------------------------#
#### Visualizing the results of the 2024 Presidential Election. 
####-------------------------------------------------------------------------#

# Sequester state and county-level map.
states_2024 <- states(cb = TRUE, year = 2023) |> 
  shift_geometry() |> 
  mutate(GEOID = as.numeric(GEOID)) |> 
  left_join(d_state_2024, by = c("GEOID" = "FIPS")) |> 
  drop_na()
counties_2024 <- counties(cb = TRUE, resolution = "5m", year = 2023) |> 
  shift_geometry() |> 
  mutate(GEOID = as.numeric(GEOID)) |> 
  left_join(d_county_2024, by = c("GEOID" = "FIPS")) |> 
  left_join(d_county_2020, by = c("GEOID" = "FIPS")) |>
  mutate(shift = (trump_pv - trump_pv_2020) * 100, 
         shift_dir = case_when(shift > 0 ~ "REP", 
                               shift < 0 ~ "DEM", 
                               TRUE ~ "No Change"),
         centroid = st_centroid(geometry), 
         centroid_long = st_coordinates(centroid)[,1],
         centroid_lat = st_coordinates(centroid)[,2],
         scale_factor = 1e4, 
         end_long = centroid_long + scale_factor * shift,
         end_lat = centroid_lat + scale_factor * shift) |>
  drop_na()
county_pop_2024 <- read_csv("PopulationEstimates.csv", show_col_types = FALSE) |> 
  mutate(FIPStxt = as.numeric(FIPStxt)) |>
  select(FIPStxt, POP_ESTIMATE_2023)
counties_2024 <- counties_2024 |> 
  left_join(county_pop_2024, by = c("GEOID" = "FIPStxt"))

# Make map of state winners. 
ggplot(states_2024, aes(fill = factor(winner))) + 
  geom_sf() + 
  scale_fill_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) + 
  theme_bw() + 
  labs(title = "2024 Presidential Election Results by State", 
       fill = "Winner") + 
  theme(legend.position = "bottom") 

# Make arrow map of county-level shifts across US. 
counties_2024 |>
  ggplot() +
  geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
  geom_curve(aes(x = centroid_long,
                 y = centroid_lat,
                 xend = end_long,
                 yend = end_lat,
                 color = shift_dir),
              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
              curvature = 0.2,  # Add a slight curve to each arrow
              size = 0.2) +
  scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
  theme_void() +
  labs(title = "Presidential Voting Shifts by County Across the US",
       subtitle = "Democratic vs. Republican Gains")
# 
# # Check county-level shifts in Pennsylvania between 2024 and 2020. 
# counties_2024 |> 
#   filter(STATE_NAME == "Pennsylvania") |> 
#   ggplot() +
#   geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
#   geom_text(aes(x = centroid_long, y = centroid_lat-1e4, label = NAME),
#             size = 2,  # Adjust size as needed
#             color = "black", hjust = 0.5, vjust = -0.5) + 
#   geom_curve(aes(x = centroid_long, 
#                  y = centroid_lat,
#                  xend = end_long, 
#                  yend = end_lat,
#                  color = shift_dir),
#              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
#              curvature = 0.2,  # Add a slight curve to each arrow
#              size = 0.3) +
#   scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
#   theme_void() +
#   labs(title = "Presidential Voting Shifts by County in Pennsylvania",
#        subtitle = "Democratic vs. Republican Gains")
# 
# # Check county-level shifts in Nevada between 2024 and 2020. 
# counties_2024 |> 
#   filter(STATE_NAME == "Nevada") |> 
#   ggplot() +
#   geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
#   geom_text(aes(x = centroid_long, y = centroid_lat-2e4, label = NAME),
#             size = 2,  # Adjust size as needed
#             color = "black", hjust = 0.5, vjust = -0.5) + 
#   geom_curve(aes(x = centroid_long, 
#                  y = centroid_lat,
#                  xend = end_long, 
#                  yend = end_lat,
#                  color = shift_dir),
#              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
#              curvature = 0.2,  # Add a slight curve to each arrow
#              size = 0.3) +
#   scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
#   theme_void() +
#   labs(title = "Presidential Voting Shifts by County in Nevada",
#        subtitle = "Democratic vs. Republican Gains")
# 
# # Check county-level shifts in Michigan between 2024 and 2020. 
# counties_2024 |> 
#   filter(STATE_NAME == "Michigan") |> 
#   ggplot() +
#   geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
#   geom_text(aes(x = centroid_long, y = centroid_lat-1e4, label = NAME),
#             size = 1.7,  # Adjust size as needed
#             color = "black", hjust = 0.5, vjust = -0.5) + 
#   geom_curve(aes(x = centroid_long, 
#                  y = centroid_lat,
#                  xend = end_long, 
#                  yend = end_lat,
#                  color = shift_dir),
#              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
#              curvature = 0.2,  # Add a slight curve to each arrow
#              size = 0.3) +
#   scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
#   theme_void() +
#   labs(title = "Presidential Voting Shifts by County in Michigan",
#        subtitle = "Democratic vs. Republican Gains")
# 
# # Check county-level shifts in Wisconsin between 2024 and 2020. 
# counties_2024 |> 
#   filter(STATE_NAME == "Wisconsin") |> 
#   ggplot() +
#   geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
#   geom_text(aes(x = centroid_long, y = centroid_lat-1.5e4, label = NAME),
#             size = 1.8,  # Adjust size as needed
#             color = "black", hjust = 0.5, vjust = -0.5) + 
#   geom_curve(aes(x = centroid_long, 
#                  y = centroid_lat,
#                  xend = end_long, 
#                  yend = end_lat,
#                  color = shift_dir),
#              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
#              curvature = 0.2,  # Add a slight curve to each arrow
#              size = 0.3) +
#   scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
#   theme_void() +
#   labs(title = "Presidential Voting Shifts by County in Wisconsin",
#        subtitle = "Democratic vs. Republican Gains")
# 
# # Check county-level shifts in Georgia between 2024 and 2020. 
# counties_2024 |> 
#   filter(STATE_NAME == "Georgia") |> 
#   ggplot() +
#   geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
#   geom_text(aes(x = centroid_long, y = centroid_lat-1e4, label = NAME),
#             size = 1.5,  # Adjust size as needed
#             color = "black", hjust = 0.5, vjust = -0.5) + 
#   geom_curve(aes(x = centroid_long, 
#                  y = centroid_lat,
#                  xend = end_long, 
#                  yend = end_lat,
#                  color = shift_dir),
#              arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
#              curvature = 0.2,  # Add a slight curve to each arrow
#              size = 0.3) +
#   scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
#   theme_void() +
#   labs(title = "Presidential Voting Shifts by County in Georgia",
#        subtitle = "Democratic vs. Republican Gains")

```

![](results.png)

![](shifts.png)

These two national maps show us the final election results and county-wise voting shits for presidential party. We see that all the battleground states that I predicted for went to Trump. The rest of the states performed how they did in the last presidential election. The shift map shows us an overwhelming shift toward the Republican Party even in supposed Democratic strongholds.

```{r echo=FALSE}

states_of_interest <- c("Pennsylvania", "Nevada", "Michigan", "Wisconsin", "Georgia")

for (state in states_of_interest) {
  plot <- counties_2024 |> 
    filter(STATE_NAME == state) |> 
    ggplot() +
    geom_sf(fill = "gray95", color = "darkgrey") +  # Base map
    geom_text(aes(x = centroid_long, y = centroid_lat - 1e4, label = NAME),
              size = 1.5,  # Adjust size as needed
              color = "black", hjust = 0.5, vjust = -0.5) + 
    geom_curve(aes(x = centroid_long, 
                   y = centroid_lat,
                   xend = end_long, 
                   yend = end_lat,
                   color = shift_dir),
               arrow = arrow(length = unit(0.1, "cm"), type = "closed"),  # Smaller arrowhead
               curvature = 0.2,  # Add a slight curve to each arrow
               size = 0.3) +
    scale_color_manual(values = c("DEM" = "steelblue3", "REP" = "tomato3")) +
    theme_void() +
    labs(title = paste("Presidential Voting Shifts by County in", state),
         subtitle = "Democratic vs. Republican Gains")

  # Save the plot as a PNG file
    ggsave(filename = paste0("voting_shifts_", state, ".png"), plot = plot, width = 8, height = 6, dpi = 300)
}

library(cowplot)

images <- lapply(states_of_interest, function(state) {
  png_file <- paste0("voting_shifts_", state, ".png")
  cowplot::ggdraw() + cowplot::draw_image(png_file, scale = 1)
})

combined_plot <- cowplot::plot_grid(plotlist = images, ncol = 2)

ggsave("combined_voting_shifts.png", combined_plot, width = 16, height = 12, dpi = 300)
```

![](combined_voting_shifts.png)

Take a look here at the county level shifts in presidential voting at the four battleground states that my model wrongly predicted plus my home state of Georgia. Like much of the United States, we see a sea of rightward shifts for the voters' choice for president. In Georgia, there is a pretty substantial enclave of leftward shifts in the southern Atlanta suburbs. If you look at this group within the context of the larger nationwide map we presented earlier, you'll see that these Atlanta suburbs are actually one of the few islands of leftward shifts for the entire nation.

## Model Accuracy

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load in available 2024 data
state_outcomes <- read_csv("state_votes_pres_2024.csv", show_col_types = FALSE)

results <- pred_sim_summary |>
  select(state, mean_dem, winner) |>
  rename(State = state,
         Winner = winner)|> 
  left_join(state_outcomes, by = c("State" = "Geographic Name")) |> 
  mutate(harris_votes = as.numeric(`Kamala D. Harris`), 
         trump_votes = as.numeric(`Donald J. Trump`),
         harris_2pv = 100 * harris_votes/(harris_votes + trump_votes),
         Error = harris_2pv - mean_dem,
         act_winner = if_else(harris_2pv>50, "Democrat", "Republican")) |>
  select(State, mean_dem, harris_2pv, Error, Winner, act_winner) |>
  rename('Actual Harris %' = harris_2pv,
         'Predicted Harris %' = mean_dem,
         'Predicted Winner' = Winner,
         'Actual Winner' = act_winner)
results

results2 <- pred_sim_summary |>
  select(state, mean_dem, winner) |>
  rename(State = state,
         pred_harris_2pv = mean_dem,
         Winner = winner)|> 
  left_join(state_outcomes, by = c("State" = "Geographic Name")) |>
  mutate(harris_votes = as.numeric(`Kamala D. Harris`), 
         trump_votes = as.numeric(`Donald J. Trump`),
         harris_2pv = 100 * harris_votes/(harris_votes + trump_votes)) |>
  select(State, pred_harris_2pv, harris_2pv) 
# Bias
bias_pred <- mean(results2$harris_2pv - results2$pred_harris_2pv)

# MSE
mse_pred <- mean((results2$harris_2pv - results2$pred_harris_2pv)^2)

# RMSE
rmse_pred <-sqrt(mean((results2$harris_2pv - results2$pred_harris_2pv)^2))

# MAE
mae_pred <-mean(abs(results2$harris_2pv - results2$pred_harris_2pv))

# Confusion Matrix
results2 <- results2 |> 
  mutate(pred_class = as.factor(case_when(pred_harris_2pv > 50 ~ "DEM", 
                                          .default = "REP")),
         result_class = as.factor(case_when(harris_2pv > 50 ~ "DEM", 
                                            .default = "REP")))

library(caret)


pred_metrics_df <- data.frame(
  Metric = c("Bias", "Mean-Squared Error (MSE)", "Root Mean-Squared Error (RMSE)", "Mean Absolute Error (MAE)"),
  Value = c(bias_pred, mse_pred, rmse_pred, mae_pred)
)

pred_metrics_df |>
  kable(caption = "Postmortem Model Metrics") |>
  kable_styling("striped")
```

Comparing the vote share predictions for each of the battleground states to the actual Democratic 2-Party vote shares, we see that the model consistently overpredicted the Democratic vote share in this election. In four of the battleground states, this had the consequence of predicting a Democratic win, when it was actually the Republican party that wont that state. In some states the error is much smaller than othes; Georgia, for example, has a much lower error than Arizona. I am curious what made the model better at predicting Democratic vote share in my home state versus Arizona or all other battleground states.

The model evaluation metric of bias corroborates this story that the model systematically overpredicted Democratic vote share. As for the other model metrics of MSE, RMSE, and MAE, we see that the actual value of error is not negligible In a race that appeared to be so close and in a game like politics, the value of a percentage point and a half matters a lot. Honestly, I do not believe my model to have been very good at predicting the outcomes this election, even if the actual vote shares were included in its 95% confidence interval.

```{r confusion_matrix, echo=FALSE}
results2$result_class <- factor(results2$result_class, levels = c("DEM", "REP"))
results2$pred_class <- factor(results2$pred_class, levels = c("DEM", "REP"))
cm_table <- table("Actual" = results2$result_class, 
                  "Prediction" = results2$pred_class)
```

```{r echo=FALSE}
cm <- confusionMatrix(cm_table)
cm

cm_matrix <- as.data.frame(cm$table)  
colnames(cm_matrix) <- c("Actual", "Prediction", "Freq") 

cm_matrix$Actual <- factor(cm_matrix$Actual, levels = c("DEM", "REP"))
cm_matrix$Prediction <- factor(cm_matrix$Prediction, levels = c("DEM", "REP"))

ggplot(cm_matrix, aes(x = Prediction, y = Actual, fill = Freq)) +
  geom_tile(color = "white") +  # Add tile borders for clarity
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "lightpink", high = "darkred") +
  labs(
    title = "Battleground Confusion Matrix + Heatmap",
    x = "Prediction",
    y = "Actual"
  ) +
  theme_minimal()+
  plot_theme()
```

Here, I use a confusion matrix to visualize the possibilities of predictions of battleground state winners and who actually won those states. You can see, according to the summary statistics, that my model accurately predicted the winner of each battleground state about 43% of the time. I am going to be blunt and say that this is pretty bad; in fact, it is worse than just guessing randomly. There were four states that I predicted the Democrats to win when the Republicans won them. I predicted 3 states that the Republicans would win, and they won those. Because the Democrats did not win any states, I could not construct a ROC-AUC curve (Receiver Operating Characteristic, Area Under the Curve), which would help visualize the efficacy of the model at predicting a winner.

## Hypotheses For What Went Wrong

I have two main hypotheses for how my model predicted a narrow Harris victory when Trump won the electoral college in a landslide. The first is that polling data, more so than other variables overpredicted Harris's predicted vote share. The second is more technical: that LASSO regression was too strict and penalized economic indicators too heavily, which would have advantaged Trump in my model.

**Polling Variables Overshot Dem Vote Share**\
What caught my eye when I was analyzing the residuals between the actual Harris vote share and the predicted Harris vote share across the battleground states was the difference among the states. As I mentioned before, Georgia had a much lower residual than Arizona and the other battleground states. This made me question just how much polling was valued in my model. How could it be that polling quality varied that much between swing states? Perhaps Georgia polls were just better. An analysis by ABC's FiveThirtyEight, however, shows that, while Georgia polls are more accurate than a lot of the nation (especially since 2016), they are pretty similar in error to Arizona and Nevada polls. Errors of polls in Michigan and Wisconsin, though, were notably higher than Georgia's (<https://abcnews.go.com/538/states-accurate-polls/story?id=115108709>). Given the variability in poll predictability, I believe that my model could have benefited from devaluing them â€” remember that my LASSO regression put quite a bit of weight onto the latest polling average as a variable.

I would also like to bring attention to another reason why overreliance on polling might be flawed. In the past three elections, it seems that pollsters have struggled to account for a nebulous Trump effect â€” where the intentions of Trump voters were hard to pick up with polling efforts. A recent analysis by the Brooking Institution had found that the final averages undercounted Trump support, but they did not necessarily overcount Harris's (<https://www.brookings.edu/articles/the-polls-underestimated-trumps-support-again/>). Nonresponse bias that is particularly higher among Trump voters might account for this, but it has been incredibly difficult to quantify â€” or even prove â€” that this effect exists.

Lastly, when I look back on the week that I solely focused on polling data, my model had predicted a Harris victory consistently. Knowing what I know now, I would have been a little bit more cautious involving polling data into my model. You can see my blog post dedicated specifically to polling data here: <https://sduggasani.github.io/2024election-blog/post/2024-09-23-week-3-polling/>.

**LASSO Was Too Strict**\
Looking back on my model, I now believe that the LASSO method of regularization was too strict and penalized features that actually did contribute to the broader model. This issue worked in tandem with the overreliance on polling data. Because we had access to so much polling data and there was fewer data on those economic indicators which I deemed "relevant", I hypothesize that LASSO exaggerated the contribution of polling data to the model while significantly diminishing the importance of other features. Research by the Pew Research Center has found that, by far, the economy was the single most important issue facing all voters this election cycle (<https://www.pewresearch.org/politics/2024/09/09/issues-and-the-2024-election/>). My model did not reflect this, largely due to the feature selection of the LASSO regression I constructed.

**Testing These Hypotheses**\
The most obvious way to test these hypotheses is to change the regularization method from LASSO to a more liberal and forgiving method, such as elastic-net. An elastic net model would minimize multi-collinearity and increase robustness. Though LASSO and Ridge regression are also useful models, the elastic net is versatile and flexibile because it incorporates both of those methods as well. This would ensure that features other than polling are still involved and contribute to the vote share while also not overindexing on polling data. I will soon be making this change and evaluating how it performs against the actual election results.

Another test I could do is a cross-validation of polling data specifically. I would split the data into those election years where Trump was running and those election years where he was not. Then, I would use cross-validation and evaluate how polling variables influenced predictions in each subset of election years. I would be curious to see if there are higher polling residuals (also underpredicting Republican performance) in those years that Trump is a candidate versus those years when he was not. This would give some shape to the "Trump effect" and would bring into question how much weight we put on polling in forecasts with "populist" candidates like Trump.

## Updating My Model

```{r echo=FALSE, cache=TRUE}
library(glmnet)
set.seed(02138)
# LASSO regression
x_train <- model.matrix(D_pv2p ~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + log(contribution_receipt_amount), data = d_train)[,-1]
y_train <- d_train$D_pv2p
elastic_net <- cv.glmnet(x_train, y_train, alpha = .5)

# Make predictions
x_test <- model.matrix(~ D_pv2p_lag1 + D_pv2p_lag2 + latest_pollav_DEM + mean_pollav_DEM + CPI + GDP_growth_quarterly + log(contribution_receipt_amount), data = d_test_simp)[,-1]
simp_pred_dem <- predict(elastic_net, newx = x_test, s = "lambda.min")
simp_pred_rep <- 100 - simp_pred_dem

# Make a battleground dataset
pred_dem <- data.frame(
  State = c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"),
  Democrat = simp_pred_dem
) |>
  rename(Democrat = lambda.min)

pred_rep <- data.frame(
  State = c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin"),
  Republican = simp_pred_rep
) |>
  rename(Republican = lambda.min)

predictions <- pred_dem |>
  left_join(pred_rep, by = "State") |>
  mutate(Winner = if_else(Democrat>Republican, "Democrat", "Republican"))

battleground_win_pred <- data.frame(state = predictions$State,
                       year = rep(2024, length(predictions$State)),
                       Democrat = predictions$Democrat,
                       Republican = predictions$Republican,
                       winner = ifelse(predictions$Winner=="Democrat", "Democrat", "Republican")) |>
  left_join(d_ec, by = c("state", "year")) |>
  select(-year, -stateab) |>
  select(state, electors, Democrat, Republican, winner)
```

```{r prediction_uncertainty, echo=FALSE}
set.seed(02138)

elastic_net <- cv.glmnet(x_train, y_train, alpha = .5)
# Predict on the test set
simp_pred_dem <- predict(elastic_net, newx = x_test, s = "lambda.min")
simp_pred_rep <- 100 - simp_pred_dem

# Residuals for the predictions
residuals <- y_train - predict(elastic_net, newx = x_train, s = "lambda.min")
residual_sd <- sd(residuals)

# Simulate predictions to generate confidence intervals
m <- 1000  
pred_mat <- replicate(m, {
  noise <- rnorm(length(simp_pred_dem), mean = 0, sd = residual_sd)
  predicted_dem <- simp_pred_dem + noise
  predicted_rep <- 100 - predicted_dem
  data.frame(Democrat = predicted_dem, Republican = predicted_rep)
}, simplify = FALSE)

# Combine all simulations
pred_sim <- do.call(rbind, pred_mat)

# Group by state and compute confidence intervals
pred_sim_summary <- pred_sim |>
  mutate(state = rep(d_test_simp$state, m)) |>
  group_by(state) |>
  summarize(
    mean_dem = mean(lambda.min),
    sd_dem = sd(lambda.min),
    lower_dem = mean_dem - 1.96 * sd_dem,
    upper_dem = mean_dem + 1.96 * sd_dem,
    mean_rep = mean(lambda.min.1),
    sd_rep = sd(lambda.min.1),
    lower_rep = mean_rep - 1.96 * sd_rep,
    upper_rep = mean_rep + 1.96 * sd_rep
  )
pred_sim_summary <- pred_sim_summary |>
  mutate(
    winner = ifelse(mean_dem > mean_rep, "Democrat", "Republican"),
    colored_state = mapply(color_state, winner, state)
  )

# Colored prediction table
pred_sim_summary |>
  select(colored_state, mean_dem, sd_dem, lower_dem, upper_dem, mean_rep, sd_rep, lower_rep, upper_rep) |>
  rename(State = colored_state) |>
  kable("html", escape = FALSE) |>
  kable_styling("striped")
```

Here, we see that, using elastic-net regularization, we can get a Trump victory in all battleground states, just like in the election.

Let's now see how the model fares with similar bias and error metrics as we reflected upon my original model with.

```{r echo=FALSE}
# Load in available 2024 data
state_outcomes <- read_csv("state_votes_pres_2024.csv", show_col_types = FALSE)

results <- pred_sim_summary |>
  select(state, mean_dem, winner) |>
  rename(State = state,
         Winner = winner)|> 
  left_join(state_outcomes, by = c("State" = "Geographic Name")) |> 
  mutate(harris_votes = as.numeric(`Kamala D. Harris`), 
         trump_votes = as.numeric(`Donald J. Trump`),
         harris_2pv = 100 * harris_votes/(harris_votes + trump_votes),
         Error = harris_2pv - mean_dem,
         act_winner = if_else(harris_2pv>50, "Democrat", "Republican")) |>
  select(State, mean_dem, harris_2pv, Error, Winner, act_winner) |>
  rename('Actual Harris %' = harris_2pv,
         'Predicted Harris %' = mean_dem,
         'Predicted Winner' = Winner,
         'Actual Winner' = act_winner)
results

results2 <- pred_sim_summary |>
  select(state, mean_dem, winner) |>
  rename(State = state,
         pred_harris_2pv = mean_dem,
         Winner = winner)|> 
  left_join(state_outcomes, by = c("State" = "Geographic Name")) |>
  mutate(harris_votes = as.numeric(`Kamala D. Harris`), 
         trump_votes = as.numeric(`Donald J. Trump`),
         harris_2pv = 100 * harris_votes/(harris_votes + trump_votes)) |>
  select(State, pred_harris_2pv, harris_2pv) 
# Bias
bias_pred <- mean(results2$harris_2pv - results2$pred_harris_2pv)

# MSE
mse_pred <- mean((results2$harris_2pv - results2$pred_harris_2pv)^2)

# RMSE
rmse_pred <-sqrt(mean((results2$harris_2pv - results2$pred_harris_2pv)^2))

# MAE
mae_pred <-mean(abs(results2$harris_2pv - results2$pred_harris_2pv))

# Confusion Matrix
results2 <- results2 |> 
  mutate(pred_class = as.factor(case_when(pred_harris_2pv > 50 ~ "DEM", 
                                          .default = "REP")),
         result_class = as.factor(case_when(harris_2pv > 50 ~ "DEM", 
                                            .default = "REP")))

library(caret)


pred_metrics_df <- data.frame(
  Metric = c("Bias", "Mean-Squared Error (MSE)", "Root Mean-Squared Error (RMSE)", "Mean Absolute Error (MAE)"),
  Value = c(bias_pred, mse_pred, rmse_pred, mae_pred)
)

pred_metrics_df |>
  kable(caption = "New Elastic-Net Model Metrics") |>
  kable_styling("striped")
```

We see here that, across all states, the bias is significantly less than the bias of the LASSO regression model. In fact, across all model evaluation metrics, the size of the error is significantly smaller. Unlike the LASSO model, this new model does not have a singular direction of bias for every state; for some states, it overpredicts for Harris, and for others, it underpredicts. The elastic-net model, however, is able to accurately predict the winner of each battleground state, where the LASSO model could not. What is interesting, though, is that the residual is actually higher in Georgia with the elastic-net model than with the LASSO model. In my final state reflection, I will scrutinize this phenomenon further.

## Conclusion

If I were to make one change to improve my model, it would be to abandon the LASSO regression for the elastic-net regression. I believe that I was too strict with my predictors in feature selection for my original model by using LASSO, and making the switch to the more forgiving elastic-net regression, has brought me to have much smaller residuals and mean errors. With elastic-net, I was able to correctly predict all battleground states.

In my final contribution to this blog, I will look further the performance of Georgia in this election. I am particularly interested in how the size of residual jumped from LASSO to elastic-net, where it generally decreased for all other states.

## Sources

Galston, Williams A. "The Polls Underestimated Trump's Support â€” Again." *Brookings*, Brookings Institution, 13 Nov. 2024, www.brookings.edu/articles/the-polls-underestimated-trumps-support-again.

Pew Research Center. "Issues and the 2024 Election." Pew Research Center: Politics and Policy, 9 Sept. 2024, www.pewresearch.org/politics/2024/09/09/issues-and-the-2024-election.

Rakich, Nathaniel. "Which States Have the Most Accurate Polls?" *FiveThirtyEight*, ABC News, 25 Oct. 2024, abcnews.go.com/538/states-accurate-polls/story?id=115108709.

Polling Data Provided by GOV 1347: Election Analytics teaching staff (which drew from the FiveThirtyEight GitHub)

Economic Data Provided by GOV 1347: Election Analytics teaching staff (which drew from the Bureau of Economic Analysis and Federal Reserve Economic Data)
